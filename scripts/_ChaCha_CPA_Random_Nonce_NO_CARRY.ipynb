{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"ChaCha-100-000-Random-Nonce-STM\"\n",
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-STM-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"ChaCha-100-000-Random-Nonce-XMEGA\"\n",
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-XMEGA-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPA_OUTPUT_FOLDER = \"_CPA_XMEGA_1_KEYS_NO_CARRY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6d931",
   "metadata": {},
   "source": [
    "# Load the content of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "NONCE_LEN_BYTES = 12\n",
    "TRACE_CNT = None\n",
    "TRACE_LEN = None\n",
    "TRACE_RANDOM_CNT = None\n",
    "CHUNK_SIZE = None\n",
    "CHUNKS_CNT = None\n",
    "LAST_CHUNK_SIZE = None\n",
    "def read_info(FOLDER):\n",
    "    \"\"\"\n",
    "    Read the info file to get the number of traces, chunk size, and trace length.\n",
    "    \"\"\"\n",
    "    global CHUNK_SIZE, LAST_CHUNK_SIZE\n",
    "    global CHUNKS_CNT\n",
    "    with open(f\"{FOLDER}/info.txt\", 'r') as file:\n",
    "        global TRACE_CNT\n",
    "        global TRACE_LEN\n",
    "        global TRACE_RANDOM_CNT\n",
    "        TRACE_CNT = int(file.readline())\n",
    "        TRACE_RANDOM_CNT = TRACE_CNT\n",
    "        CHUNK_SIZE = int(file.readline())\n",
    "        TRACE_LEN = int(file.readline())\n",
    "        \n",
    "    CHUNKS_CNT = math.ceil(TRACE_CNT / CHUNK_SIZE)\n",
    "    LAST_CHUNK_SIZE = TRACE_CNT - (CHUNKS_CNT - 1)* CHUNK_SIZE\n",
    "    print(f\"TRACE_CNT = {TRACE_CNT}\")   \n",
    "    print(f\"CHUNK_SIZE = {CHUNK_SIZE}\")   \n",
    "    print(f\"LAST_CHUNK_SIZE = {LAST_CHUNK_SIZE}\")   \n",
    "    print(f\"CHUNKS_CNT = {CHUNKS_CNT}\")   \n",
    "    print(f\"TRACE_RANDOM_CNT = {TRACE_RANDOM_CNT}\")   \n",
    "    print(f\"TRACE_LEN = {TRACE_LEN}\")\n",
    "    \n",
    "    \n",
    "read_info(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f88a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRACE_CNT = 500_000\n",
    "# CHUNKS_CNT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NONCES = None\n",
    "def read_nonces(FOLDER):\n",
    "    \"\"\"\n",
    "    Read the nonces from the binary files in the specified folder.\n",
    "    The nonces are stored in chunks, and the function concatenates them into a single array.\n",
    "    \"\"\"\n",
    "    global NONCES\n",
    "    global CHUNK_SIZE, LAST_CHUNK_SIZE\n",
    "    global CHUNKS_CNT\n",
    "    global TRACE_CNT\n",
    "    # Calculate the number of chunks\n",
    "    \n",
    "    nonces_list = []\n",
    "    for chunk_index in range(CHUNKS_CNT):\n",
    "        chunk_folder = os.path.join(FOLDER, f\"chunk_{chunk_index}\")\n",
    "        chunk_file = os.path.join(chunk_folder, \"nonces_random.bin\")\n",
    "        \n",
    "        if os.path.exists(chunk_file):\n",
    "            with open(chunk_file, 'rb') as file:\n",
    "                byte_array = file.read()\n",
    "            \n",
    "            if chunk_index != CHUNKS_CNT-1:\n",
    "                chunk = np.frombuffer(byte_array, dtype=np.uint8).reshape((CHUNK_SIZE, NONCE_LEN_BYTES))\n",
    "            else:\n",
    "                chunk = np.frombuffer(byte_array, dtype=np.uint8).reshape((LAST_CHUNK_SIZE, NONCE_LEN_BYTES))\n",
    "\n",
    "            nonces_list.append(chunk)\n",
    "        else:\n",
    "            print(f\"Chunk file {chunk_file} does not exist.\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    NONCES = np.vstack(nonces_list)\n",
    "        \n",
    "    # Desired shape\n",
    "    desired_shape = (TRACE_CNT, NONCE_LEN_BYTES)\n",
    "\n",
    "    # Check if the array has the desired shape\n",
    "    if NONCES.shape != desired_shape:\n",
    "        # Trim the array to the desired shape\n",
    "        print(f\"NONCES.shape != desired_shape - {NONCES.shape} != {desired_shape}\")\n",
    "        NONCES = NONCES[:desired_shape[0], :desired_shape[1]]\n",
    "        print(f\"Trimming NONCES to {NONCES.shape}\")\n",
    "        \n",
    "    nonce = NONCES[0]\n",
    "\n",
    "    # Convert each byte to its hexadecimal representation and join them\n",
    "    hex_representation = ''.join(f'{byte:02x}' for byte in nonce[:12])\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Nonce[0] = {hex_representation}\")\n",
    "    \n",
    "    \n",
    "read_nonces(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ca545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the correct key\n",
    "CORRECT_KEY = np.fromfile(f\"{FOLDER}/key.bin\", dtype=np.uint8)\n",
    "hex_key = ''.join(f'{byte:02x}' for byte in CORRECT_KEY)\n",
    "print(f\"Correct Key: {hex_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACES_RANDOM = None\n",
    "\n",
    "def load_traces_random(folder):\n",
    "    \"\"\"\n",
    "    Load traces from the specified folder. \n",
    "    The traces are stored in binary files (chunks, defined in info.txt file),\n",
    "    and the function concatenates them into a single array.\n",
    "    \"\"\"\n",
    "    global TRACES_RANDOM, CHUNK_SIZE, LAST_CHUNK_SIZE, CHUNKS_CNT, TRACE_CNT\n",
    "\n",
    "    traces_list = []\n",
    "    for chunk_index in range(CHUNKS_CNT):\n",
    "        chunk_folder = os.path.join(folder, f\"chunk_{chunk_index}\")\n",
    "        chunk_file = os.path.join(chunk_folder, \"traces_random.bin\")\n",
    "\n",
    "        if os.path.exists(chunk_file):\n",
    "            with open(chunk_file, 'rb') as file:\n",
    "                byte_array = file.read()\n",
    "\n",
    "            if chunk_index != CHUNKS_CNT-1:\n",
    "                chunk_traces = np.frombuffer(byte_array, dtype=np.uint16).reshape((CHUNK_SIZE, TRACE_LEN))\n",
    "            else:\n",
    "                chunk_traces = np.frombuffer(byte_array, dtype=np.uint16).reshape((LAST_CHUNK_SIZE, TRACE_LEN))\n",
    "\n",
    "            traces_list.append(chunk_traces)\n",
    "        else:\n",
    "            print(f\"Chunk file {chunk_file} does not exist.\")\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    TRACES_RANDOM = np.vstack(traces_list)\n",
    "\n",
    "load_traces_random(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdac7e9",
   "metadata": {},
   "source": [
    "# Adjust size of traces if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crop every trace (row) to sample range 0..1499\n",
    "# TRACE_CNT = 5000\n",
    "# TRACES_RANDOM = TRACES_RANDOM[:TRACE_CNT, :]\n",
    "# # TRACES_RANDOM = TRACES_RANDOM[:, 0:8_000]\n",
    "\n",
    "# NONCES= NONCES[:TRACE_CNT, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TRACES shape = {TRACES_RANDOM.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc466e08",
   "metadata": {},
   "source": [
    "# Helper functions and constants setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHACHA_KEY_LEN_BYTES = 32\n",
    "VALUES_IN_BYTE = 256 #0...255\n",
    "\n",
    "# constant sigma - 16B - \"expand 32-byte k\" transformed to bytes\n",
    "fixed_sigma = bytearray(\"expand 32-byte k\", \"utf-8\")\n",
    "print(hex(fixed_sigma[4]))\n",
    "\n",
    "# Hamming weight table for 8-bit values\n",
    "# This table is used to calculate the Hamming weight of a byte value.\n",
    "hamming_weight = [\n",
    "   0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, \n",
    "   2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2, 3, 2, 3, 3, 4, \n",
    "   2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, \n",
    "   5, 6, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, \n",
    "   3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, \n",
    "   4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, \n",
    "   1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, \n",
    "   4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, \n",
    "   4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, \n",
    "   3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, \n",
    "   4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, \n",
    "   6, 7, 4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8]\n",
    "\n",
    "def corr2_coeff(A, B):\n",
    "    \"\"\"\n",
    "    Calculate the correlation coefficient between two matrices A and B.\n",
    "    Each row of A and B is treated as a separate variable.\n",
    "    The function returns a matrix of correlation coefficients.\n",
    "    \"\"\"\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:, None]\n",
    "    B_mB = B - B.mean(1)[:, None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1)\n",
    "    ssB = (B_mB**2).sum(1)\n",
    "    \n",
    "        # Locate rows with zero sum of squares\n",
    "    zero_ssA_indices = np.where(ssA == 0)[0]\n",
    "    zero_ssB_indices = np.where(ssB == 0)[0]\n",
    "    \n",
    "    # Avoid division by zero by setting problematic rows to NaN\n",
    "    ssA[zero_ssA_indices] = np.nan\n",
    "    ssB[zero_ssB_indices] = np.nan\n",
    "    \n",
    "    one_part = np.sqrt(np.dot(ssA[:, None],ssB[None]))\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return np.dot(A_mA, B_mB.T) / one_part\n",
    "\n",
    "def leftRotate16(n):\n",
    "    return ((n << 16) | (n >> 16)) & 0xFFFFFFFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86443960",
   "metadata": {},
   "source": [
    "## Functions to obtain the K[4-15] and K[20-31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rotated_previous_intermediate_values_vector(size, \n",
    "                                                          trace_shift, \n",
    "                                                          column_index, \n",
    "                                                          obtained_key_byte_array):\n",
    "    \"\"\"\n",
    "    Calculate the rotated previous intermediate values vector for a given column of the ChaCha20 state.\n",
    "    This function computes the intermediate values based on the provided key byte array and the fixed sigma value.\n",
    "\n",
    "    Parameters:\n",
    "    - size: The number of intermediate values to calculate.\n",
    "    - trace_shift: The number of traces to shift.\n",
    "    - column_index: The column index to use for the calculation (must be greater than 0).\n",
    "    - obtained_key_byte_array: The byte array of the obtained key.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the calculated intermediate values.\n",
    "    \"\"\"\n",
    "    if column_index == 0:\n",
    "        raise ValueError(\"Column index must be greater than 0.\")\n",
    "        \n",
    "    if size + trace_shift > len(NONCES):\n",
    "        raise ValueError(\"Trace shift must be smaller.\")\n",
    "\n",
    "\n",
    "    intermediate_values = np.empty(size, dtype=np.uint32)  # Store 32-bit intermediate values\n",
    "    \n",
    "    # Extract 4 corresponding bytes from the key and form a 32-bit word (LSB first)\n",
    "    key_word = (\n",
    "        obtained_key_byte_array[column_index * 4 + 0] |\n",
    "        (obtained_key_byte_array[column_index * 4 + 1] << 8) |\n",
    "        (obtained_key_byte_array[column_index * 4 + 2] << 16) |\n",
    "        (obtained_key_byte_array[column_index * 4 + 3] << 24)\n",
    "    )\n",
    "\n",
    "    # Calculate the intermediate value: nonce_word ^ (key_word + sigma_word)\n",
    "    sigma_word = (\n",
    "        fixed_sigma[column_index * 4 + 0] |\n",
    "        (fixed_sigma[column_index * 4 + 1] << 8) |\n",
    "        (fixed_sigma[column_index * 4 + 2] << 16) |\n",
    "        (fixed_sigma[column_index * 4 + 3] << 24)\n",
    "    )\n",
    "        \n",
    "    arr_index = 0\n",
    "    for i in range(trace_shift, trace_shift + size):\n",
    "        # Extract 4 corresponding bytes from NONCES and form a 32-bit word (LSB first)\n",
    "        nonce_word = (\n",
    "            NONCES[i][(column_index - 1) * 4 + 0] |\n",
    "            (NONCES[i][(column_index - 1) * 4 + 1] << 8) |\n",
    "            (NONCES[i][(column_index - 1) * 4 + 2] << 16) |\n",
    "            (NONCES[i][(column_index - 1) * 4 + 3] << 24)\n",
    "        )\n",
    "        intermediate_values[arr_index] = nonce_word ^ (key_word + sigma_word)\n",
    "        intermediate_values[arr_index] = leftRotate16(intermediate_values[arr_index])\n",
    "        arr_index += 1\n",
    "\n",
    "    # Convert the intermediate values to numpy array of 4 x uint8\n",
    "    intermediate_values = np.array(\n",
    "        [\n",
    "            (\n",
    "                (intermediate_values[i]        & 0xFF),\n",
    "                ((intermediate_values[i] >>  8) & 0xFF),\n",
    "                ((intermediate_values[i] >> 16) & 0xFF),\n",
    "                ((intermediate_values[i] >> 24) & 0xFF)\n",
    "            ) for i in range(size)\n",
    "        ], dtype=np.uint8)\n",
    "    \n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICATION: NO CARRY\n",
    "def calculate_row_of_intermediate_values_for_key_byte_4_to_15(nonce_row, \n",
    "                                                              byte_index, \n",
    "                                                              obtained_key_byte_array=None):\n",
    "    \"\"\"\n",
    "    Calculate one row of intermediate values for a specific key byte (4-15) based on the nonce row and fixed sigma.\n",
    "\n",
    "    Parameters:\n",
    "        nonce_row (np.ndarray): A single row of the NONCES array (shape: [NONCE_LEN_BYTES]).\n",
    "        byte_index (int): Index of the key byte (4-15).\n",
    "        prev_key_byte (int or None): Value of the previous key byte for carry calculation. None if not needed.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Intermediate values for all possible key byte values (shape: [256]).\n",
    "    \"\"\"\n",
    "\n",
    "    if 4 <= byte_index <= 15:\n",
    "        nonce_byte = nonce_row[byte_index - 4] # Adjusted index for nonce byte\n",
    "        \n",
    "        # Calculate possible values based on the nonce byte and fixed sigma\n",
    "        possible_values_vector = np.array([\n",
    "            nonce_byte ^ ((key_candidate + fixed_sigma[byte_index]) & 0xFF)\n",
    "            for key_candidate in range(256)\n",
    "        ], dtype=np.uint8)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Byte index must be in the range 4-15 or 20-31.\")\n",
    "\n",
    "    # Convert to hamming weights (or other leakage model)\n",
    "    intermediate_values = np.array([hamming_weight[value] for value in possible_values_vector], dtype=np.uint16)\n",
    "\n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICATION: NO CARRY\n",
    "def calculate_row_of_intermediate_values_for_key_byte_20_to_31(byte_index, \n",
    "                                                               previous_intermediate_value_array, \n",
    "                                                               obtained_key_byte_array=None):\n",
    "    \"\"\"\n",
    "    Calculate one row of intermediate values for a specific key byte (20-31) based on the previous intermediate values and fixed sigma.\n",
    "\n",
    "    Parameters:\n",
    "        byte_index (int): Index of the key byte (20-31).\n",
    "        previous_intermediate_value_array (np.ndarray): Array of previous intermediate values (shape: [4]).\n",
    "        obtained_key_byte_array (np.ndarray): Array of obtained key bytes (shape: [32]).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Intermediate values for all possible key byte values (shape: [256]).\n",
    "    \"\"\"\n",
    "    if 20 <= byte_index <= 31:\n",
    "        possible_values_vector = np.array([\n",
    "            (( key_candidate + previous_intermediate_value_array[byte_index % 4]) ^ obtained_key_byte_array[byte_index - 16]) & 0xFF\n",
    "            for key_candidate in range(256)\n",
    "        ], dtype=np.uint8)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Byte index must be in the range 20-31.\")\n",
    "    \n",
    "    # Convert to hamming weights\n",
    "    intermediate_values = np.array([hamming_weight[value] for value in possible_values_vector], dtype=np.uint16)\n",
    "\n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31116d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_matrix_col_1_to_3(byte_index, \n",
    "                                            num_traces, \n",
    "                                            trace_shift=0, \n",
    "                                            obtained_key_array = CORRECT_KEY,\n",
    "                                            previous_intermediate_values = None):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix for a specific key from ChaCha20 state columns (1-3).\n",
    "\n",
    "    Parameters:\n",
    "        byte_index (int): Index of the key byte (4-15 or 20-31).\n",
    "        num_traces (int): Number of traces to use for the calculation.\n",
    "        trace_shift (int): Number of traces to shift for the calculation. Default is 0.\n",
    "        obtained_key_array (np.ndarray): Array of obtained key bytes (shape: [32]).\n",
    "        previous_intermediate_values (np.ndarray): Array of previous intermediate values (shape: [TRACE_LEN]).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Correlation matrix of shape (256, TRACE_LEN).\n",
    "    \"\"\"\n",
    "    global NONCES, TRACES_RANDOM\n",
    "    \n",
    "    # Ensure the number of traces does not exceed available traces\n",
    "    max_traces = TRACES_RANDOM.shape[0]\n",
    "    if trace_shift >= max_traces:\n",
    "        raise ValueError(f\"trace_shift ({trace_shift}) exceeds the number of available traces ({max_traces}).\")\n",
    "    \n",
    "    num_traces = min(num_traces, max_traces - trace_shift)\n",
    "\n",
    "    # Slice the traces and nonces to the specified number of traces\n",
    "    traces = TRACES_RANDOM[trace_shift:num_traces + trace_shift, :]\n",
    "    nonces = NONCES[trace_shift:num_traces + trace_shift, :]\n",
    "\n",
    "    # Placeholder for intermediate values (to be calculated later)\n",
    "    L_matrix = np.empty((num_traces, 256), dtype=np.float32)\n",
    "\n",
    "    # Check if previous intermediate values are needed and if calculated\n",
    "    if byte_index >= 20 and previous_intermediate_values is None:\n",
    "        raise ValueError(\"Previous intermediate values are required for byte index >= 20.\")\n",
    "\n",
    "    # Calculate intermediate values for each trace\n",
    "    for trace_idx in range(num_traces):\n",
    "        if 4 <= byte_index <= 15:\n",
    "            L_matrix[trace_idx, :] = calculate_row_of_intermediate_values_for_key_byte_4_to_15(\n",
    "                nonces[trace_idx], byte_index, obtained_key_array\n",
    "            )\n",
    "        elif 20 <= byte_index <= 31:\n",
    "            L_matrix[trace_idx, :] = calculate_row_of_intermediate_values_for_key_byte_20_to_31(\n",
    "                byte_index, previous_intermediate_values[trace_idx + trace_shift] , obtained_key_array\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Byte index must be in the range 4-15 or 20-31.\")\n",
    "\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = corr2_coeff(L_matrix.T, traces.T)\n",
    "\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb9898",
   "metadata": {},
   "source": [
    "## Correlation matrixes calculation using the correct key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store intermediate values for columns 1, 2, and 3\n",
    "PREVIOUS_INTERMEDIATE_VALUES = {}\n",
    "\n",
    "def calculate_all_rotated_previous_intermediate_values(size, trace_shift, obtained_key_byte_array):\n",
    "    \"\"\"\n",
    "    Calculate rotated previous intermediate values for columns 1, 2, and 3\n",
    "    and store them in a global variable for later use.\n",
    "    \n",
    "    Parameters:\n",
    "        size (int): Number of rows (traces/nonces) to process.\n",
    "        trace_shift (int): Number of traces to shift for the calculation.\n",
    "        obtained_key_byte_array (np.ndarray): Array of key bytes (shape: [CHACHA_KEY_LEN_BYTES]).\n",
    "    \"\"\"\n",
    "    global PREVIOUS_INTERMEDIATE_VALUES\n",
    "    \n",
    "    for column_index in range(1, 4):  # Columns 1, 2, and 3\n",
    "        print(f\"Calculating rotated previous intermediate values for column {column_index}...\")\n",
    "        PREVIOUS_INTERMEDIATE_VALUES[column_index] = calculate_rotated_previous_intermediate_values_vector(\n",
    "            size=size,\n",
    "            trace_shift=trace_shift,\n",
    "            column_index=column_index,\n",
    "            obtained_key_byte_array=obtained_key_byte_array\n",
    "        )\n",
    "    print(\"Rotated previous intermediate values for columns 1, 2, and 3 have been calculated and stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f23925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_matrix_for_byte_range(\n",
    "    byte_range, \n",
    "    num_traces, \n",
    "    trace_shift=0, \n",
    "    obtained_key_array=CORRECT_KEY, \n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix for a specified range of bytes.\n",
    "\n",
    "    Parameters:\n",
    "        byte_range (tuple): A tuple specifying the start and end byte indices (inclusive).\n",
    "        num_traces (int): Number of traces to use for the calculation.\n",
    "        trace_shift (int): Number of traces to shift for the calculation. Default is 0.\n",
    "        obtained_key_array (np.ndarray): Array of obtained key bytes (shape: [32]).\n",
    "    \"\"\"\n",
    "    global CORRELATION_MATRIXES, PREVIOUS_INTERMEDIATE_VALUES\n",
    "    start_byte, end_byte = byte_range\n",
    "    \n",
    "    for byte_index in range(start_byte, end_byte + 1):\n",
    "        print(f\"Calculating correlation matrix for key byte {byte_index}...\")\n",
    "\n",
    "        if byte_index < 4:\n",
    "            # Key bytes 0–3 are not part of the main recovery process\n",
    "            print(f\"Skipping key byte {byte_index} (not part of recovery).\")\n",
    "            continue\n",
    "\n",
    "        if 4 <= byte_index <= 15:\n",
    "            # Calculate correlation matrix for key bytes 4–15\n",
    "            CORRELATION_MATRIXES[byte_index] = calculate_correlation_matrix_col_1_to_3(\n",
    "                byte_index=byte_index,\n",
    "                num_traces=num_traces,\n",
    "                trace_shift=trace_shift,\n",
    "                obtained_key_array=obtained_key_array\n",
    "            )\n",
    "        elif 20 <= byte_index <= 31:\n",
    "            # Calculate correlation matrix for key bytes 20–31\n",
    "            if PREVIOUS_INTERMEDIATE_VALUES is None:\n",
    "                raise ValueError(\"Previous intermediate values are required for byte index >= 20.\")\n",
    "\n",
    "            # Determine the column index based on the byte index\n",
    "            column_index = (byte_index - 20) // 4 + 1  # Maps 20–23 -> 1, 24–27 -> 2, 28–31 -> 3\n",
    "\n",
    "            # Pass the corresponding intermediate values to the function\n",
    "            CORRELATION_MATRIXES[byte_index] = calculate_correlation_matrix_col_1_to_3(\n",
    "                byte_index=byte_index,\n",
    "                num_traces=num_traces,\n",
    "                trace_shift=trace_shift,\n",
    "                obtained_key_array=obtained_key_array,\n",
    "                previous_intermediate_values=PREVIOUS_INTERMEDIATE_VALUES[column_index]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Skipping unsupported key byte {byte_index}.\")\n",
    "\n",
    "    print(\"Correlation matrices calculation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0dfe3",
   "metadata": {},
   "source": [
    "# Correlation matrixes processing (Plot, (Sub)Key retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f67ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of traces to use for the calculation\n",
    "# This can be adjusted based on the available data and requirements\n",
    "TRACES_USED = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ad344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrices for key bytes 0–31\n",
    "CORRELATION_MATRIXES = {}\n",
    "\n",
    "# Define the ranges, in which the min. correlation is searched, for all 32 bytes as a list of tuples (start, end)\n",
    "BYTE_RANGES = []\n",
    "\n",
    "# Global array to store the found key byte values\n",
    "FOUND_KEY_BYTES = [None] * 32   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae169d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_correlation_matrix(\n",
    "    byte_index, \n",
    "    show_plots=True, \n",
    "    save_plots=False, \n",
    "    zoom_plot=False,\n",
    "    list_correlations=False,\n",
    "    correct_key_bytes=CORRECT_KEY,\n",
    "    output_folder=\"plots\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the correlation matrix for a given key byte index, optionally display and save the plots,\n",
    "    and save the found key byte value to a global array.\n",
    "\n",
    "    Parameters:\n",
    "        byte_index (int): The key byte index being processed.\n",
    "        show_plots (bool): Whether to display the plot.\n",
    "        save_plots (bool): Whether to save the plot as an image.\n",
    "        zoom_plot (bool): Whether to zoom into the range defined by `byte_ranges`.\n",
    "        list_correlations (bool): Whether to list the correlations for all key candidates.\n",
    "        correct_key_bytes (np.ndarray): The correct key bytes for comparison.\n",
    "        output_folder (str): Folder to save the plots if `save_plots` is True.\n",
    "    \"\"\"\n",
    "    global FOUND_KEY_BYTES, CORRELATION_MATRIXES, BYTE_RANGES, VALUES_IN_BYTE\n",
    "\n",
    "    if byte_index < 0 or byte_index >= 32:\n",
    "        raise ValueError(\"Byte index must be in the range 0-31.\")\n",
    "\n",
    "    # Determine the correct key byte value for the given byte index\n",
    "    correct_key_byte = correct_key_bytes[byte_index]\n",
    "\n",
    "    # Get the correlation matrix cut-out for the current byte index\n",
    "    correlation_matrix_cutout = CORRELATION_MATRIXES[byte_index][:, BYTE_RANGES[byte_index][0]:BYTE_RANGES[byte_index][1]]\n",
    "\n",
    "    # Find the minimum correlation value and its corresponding key candidate\n",
    "    corr_min = np.unravel_index(correlation_matrix_cutout.argmin(), correlation_matrix_cutout.shape)\n",
    "    obtained_key_min = corr_min[0]\n",
    "    min_value = correlation_matrix_cutout[corr_min]\n",
    "    FOUND_KEY_BYTES[byte_index] = obtained_key_min\n",
    "\n",
    "    # Find the correlation of the correct key byte\n",
    "    correct_key_correlation = np.min(correlation_matrix_cutout[correct_key_byte])\n",
    "\n",
    "    output = f\"{byte_index:>2} & {BYTE_RANGES[byte_index]} & \\\\texttt{{{format(obtained_key_min, '#04X').replace('0X', '0x')}}} ({min_value:.5f}) & \\\\texttt{{{format(correct_key_byte, '#04X').replace('0X', '0x')}}} ({correct_key_correlation:.5f})\"\n",
    "    print(output)\n",
    "    \n",
    "    if list_correlations:\n",
    "        # Calculate the minimum correlation value for each key candidate\n",
    "        min_correlations = []\n",
    "        for candidate in range(VALUES_IN_BYTE):\n",
    "            min_corr = np.min(correlation_matrix_cutout[candidate])\n",
    "            min_correlations.append((min_corr, candidate))\n",
    "        \n",
    "        # Sort the candidates based on their minimum correlation values\n",
    "        min_correlations.sort()  # Sort by the first element of the tuple (min_corr)\n",
    "        \n",
    "        # Print the top 10 sorted candidates and their correlation values\n",
    "        print(f\"Top 10 sorted candidates for key byte {byte_index}:\")\n",
    "        for i in range(min(10, len(min_correlations))):  # Ensure we don't exceed the list length\n",
    "            print(f\"Candidate: {hex(min_correlations[i][1])}, Correlation: {min_correlations[i][0]}\")\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)  # Ensure the folder exists\n",
    "        output_file = os.path.join(output_folder, f\"correlation_results_byte_{byte_index}.txt\")\n",
    "\n",
    "        with open(output_file, \"w\") as file:\n",
    "            for i in range(0, len(min_correlations)):\n",
    "                file.write(f\"Candidate: {hex(min_correlations[i][1])}, Correlation: {min_correlations[i][0]}\\n\")\n",
    "        \n",
    "    if not show_plots and not save_plots:\n",
    "        return\n",
    "\n",
    "    # Prepare the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plot all key candidates\n",
    "    for corr_i in range(VALUES_IN_BYTE):\n",
    "        if corr_i != correct_key_byte and corr_i != obtained_key_min:\n",
    "            plt.plot(CORRELATION_MATRIXES[byte_index][corr_i], 'k.', alpha=0.3)\n",
    "\n",
    "    # Highlight the correct key byte and the guessed key byte\n",
    "    plt.plot(CORRELATION_MATRIXES[byte_index][obtained_key_min], 'r.', label=f\"Guessed Key = {hex(obtained_key_min)}\")\n",
    "    plt.plot(CORRELATION_MATRIXES[byte_index][correct_key_byte], 'g.', label=f\"Correct Key = {hex(correct_key_byte)}\")\n",
    "\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"Trace Sample\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    start_col, end_col = BYTE_RANGES[byte_index]\n",
    "    plt.axvline(x=start_col, color='b', linestyle='--', label='Search Range')\n",
    "    plt.axvline(x=end_col, color='b', linestyle='--')\n",
    "    \n",
    "    # Save the plot if required\n",
    "    if save_plots:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        plot_path = os.path.join(output_folder, f\"key_byte_{byte_index:02}.png\")\n",
    "        plt.savefig(plot_path, bbox_inches='tight', dpi=600)\n",
    "        print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "    # Show the plot\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Add zoomed range if enabled\n",
    "    if zoom_plot:\n",
    "        # Prepare the plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Plot all key candidates\n",
    "        for corr_i in range(VALUES_IN_BYTE):\n",
    "            if corr_i != correct_key_byte and corr_i != obtained_key_min:\n",
    "                plt.plot(CORRELATION_MATRIXES[byte_index][corr_i], 'k.', alpha=0.3)\n",
    "\n",
    "        # Highlight the correct key byte and the guessed key byte\n",
    "        plt.plot(CORRELATION_MATRIXES[byte_index][obtained_key_min], 'r.', label=f\"Guessed Key = {hex(obtained_key_min)}\")\n",
    "        plt.plot(CORRELATION_MATRIXES[byte_index][correct_key_byte], 'g.', label=f\"Correct Key = {hex(correct_key_byte)}\")\n",
    "\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.xlabel(\"Trace Sample\")\n",
    "        plt.ylabel(\"Correlation\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.xlim(start_col, end_col)\n",
    "\n",
    "        # Show the plot\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "\n",
    "        # Save the plot if required\n",
    "        if save_plots:\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            plot_path = os.path.join(output_folder, f\"key_byte_{byte_index:02}_zoomed.png\")\n",
    "            plt.savefig(plot_path, bbox_inches='tight', dpi=600)\n",
    "            print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "        # Close the plot to free memory\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67559833",
   "metadata": {},
   "source": [
    "## Execution of the anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae70292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to calculate and store intermediate values used for extraction of key bytes in second row\n",
    "# with more complex leakage function - PREVIOUS_INTERMEDIATE_VALUES\n",
    "calculate_all_rotated_previous_intermediate_values(\n",
    "    size=TRACES_USED,\n",
    "    trace_shift=0,\n",
    "    obtained_key_byte_array=CORRECT_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the specified byte range\n",
    "# This will calculate the correlation matrix for key bytes 4 to 15 and 20 to 31\n",
    "# and store them in the CORRELATION_MATRIXES dictionary\n",
    "calculate_correlation_matrix_for_byte_range(\n",
    "    byte_range=(0, 31), \n",
    "    num_traces=TRACES_USED, \n",
    "    trace_shift=0, \n",
    "    obtained_key_array=CORRECT_KEY, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3467620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the byte ranges for each key byte (0-31) as a list of tuples (start, end)\n",
    "# The ranges are defined based on the analysis of the correlation matrices.\n",
    "# The ranges are adjusted to focus on the most relevant samples for each key byte.\n",
    "# The ranges are defined as (start, end) tuples, where start and end are the sample indices.\n",
    "BYTE_RANGES_STM = [\n",
    "    (0, TRACE_LEN),  # Byte 0\n",
    "    (0, TRACE_LEN),  # Byte 1\n",
    "    (0, TRACE_LEN),  # Byte 2\n",
    "    (0, TRACE_LEN),  # Byte 3\n",
    "    \n",
    "    (500, 850),# Byte 4\n",
    "    (500, 850),# Byte 5\n",
    "    (500, 850),# Byte 6\n",
    "    (500, 850),# Byte 7\n",
    "    \n",
    "    (570, 920),# Byte 8\n",
    "    (570, 920),# Byte 9\n",
    "    (570, 920),# Byte 10\n",
    "    (570, 920),# Byte 11\n",
    "    \n",
    "    (570, 920),# Byte 12\n",
    "    (570, 920),# Byte 13\n",
    "    (570, 920),# Byte 14 \n",
    "    (570, 920),# Byte 15\n",
    "    \n",
    "    (0, TRACE_LEN),# Byte 16\n",
    "    (0, TRACE_LEN),# Byte 17\n",
    "    (0, TRACE_LEN),# Byte 18\n",
    "    (0, TRACE_LEN),# Byte 19\n",
    "    \n",
    "    (500, 950),# Byte 20\n",
    "    (500, 950),# Byte 21\n",
    "    (500, 950),# Byte 22\n",
    "    (500, 950),# Byte 23\n",
    "    \n",
    "    (550, 950),# Byte 24\n",
    "    (550, 950),# Byte 25\n",
    "    (550, 950),# Byte 26\n",
    "    (550, 950),# Byte 27\n",
    "    \n",
    "    (550, 950),# Byte 28\n",
    "    (550, 950),# Byte 29\n",
    "    (550, 950),# Byte 30\n",
    "    (550, 950),# Byte 31\n",
    "]\n",
    "\n",
    "BYTE_RANGES = BYTE_RANGES_STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f95899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the byte ranges for each key byte (0-31) as a list of tuples (start, end)\n",
    "# The ranges are defined based on the analysis of the correlation matrices.\n",
    "# The ranges are adjusted to focus on the most relevant samples for each key byte.\n",
    "# The ranges are defined as (start, end) tuples, where start and end are the sample indices.\n",
    "BYTE_RANGES_XMEGA = [\n",
    "    (0, TRACE_LEN),  # Byte 0\n",
    "    (0, TRACE_LEN),  # Byte 1\n",
    "    (0, TRACE_LEN),  # Byte 2\n",
    "    (0, TRACE_LEN),  # Byte 3\n",
    "    \n",
    "    (4700, 5100),# Byte 4\n",
    "    (4700, 5100),# Byte 5\n",
    "    (4770, 5100),# Byte 6\n",
    "    (4700, 5100),# Byte 7\n",
    "    \n",
    "    (7100, 7500),# Byte 8 (7100, 7200)\n",
    "    (7100, 7500),# Byte 9\n",
    "    (7100, 7500),# Byte 10\n",
    "    (7100, 7500),# Byte 11 (7100, 7150)\n",
    "    \n",
    "    (9450, 9800),# Byte 12\n",
    "    (9450, 9800),#(9450, 9550),# Byte 13\n",
    "    (9550, 9600),#(9450, 9550),# Byte 14 (9480, 9515), found somehow\n",
    "    (9450, 9520),#(9450, 9520),# Byte 15\n",
    "    \n",
    "    (0, TRACE_LEN),# Byte 16\n",
    "    (0, TRACE_LEN),# Byte 17\n",
    "    (0, TRACE_LEN),# Byte 18\n",
    "    (0, TRACE_LEN),# Byte 19\n",
    "    \n",
    "    (5500, 5760),# Byte 20\n",
    "    (5500, 5760),# Byte 21 (5000, 6500), - 14 instead 13\n",
    "    (5500, 5750),# Byte 22\n",
    "    (5500, 5760),# Byte 23\n",
    "    \n",
    "    (7700, 8160),# Byte 24\n",
    "    (7700, 8160),# Byte 25\n",
    "    (7700, 8150),# Byte 26\n",
    "    (7700, 8160),# Byte 27\n",
    "    \n",
    "    (9900, 11000),# Byte 28\n",
    "    (9400, 11000),# Byte 29\n",
    "    (9400, 11000),# Byte 30\n",
    "    (9400, 11000),# Byte 31\n",
    "]\n",
    "\n",
    "BYTE_RANGES = BYTE_RANGES_XMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the key guesses for bytes 4 to 15 and 20 to 31\n",
    "# The key guesses are stored in the FOUND_KEY_BYTES global array.\n",
    "# The correct key bytes are used for comparison and output formatting.\n",
    "for byte_index in chain(range(4, 16), range(20, 32)):\n",
    "    process_and_plot_correlation_matrix(\n",
    "        byte_index=byte_index,\n",
    "        show_plots=False,\n",
    "        save_plots=False,\n",
    "        zoom_plot=False,\n",
    "        list_correlations=False,\n",
    "        correct_key_bytes=CORRECT_KEY,\n",
    "        output_folder=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329bb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the key guesses for bytes 4 to 15 and 20 to 31 together with the plots\n",
    "# The key guesses are stored in the FOUND_KEY_BYTES global array.\n",
    "# Plots are saved to the specified output folder.\n",
    "for byte_index in chain(range(4, 16), range(20, 32)):\n",
    "    process_and_plot_correlation_matrix(\n",
    "        byte_index=byte_index,\n",
    "        show_plots=False,\n",
    "        save_plots=True,\n",
    "        zoom_plot=True,\n",
    "        list_correlations=True,\n",
    "        correct_key_bytes=CORRECT_KEY,\n",
    "        output_folder=CPA_OUTPUT_FOLDER\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6397",
   "metadata": {},
   "source": [
    "## Success rate in bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges of key bytes to analyze\n",
    "KEY_BYTE_RANGES = list(range(4, 16)) + list(range(20, 32))\n",
    "\n",
    "# Initialize counters for correct bytes and correct bits\n",
    "correct_key_bytes = 0\n",
    "correct_key_bits = 0\n",
    "\n",
    "# Iterate over the specified key byte ranges\n",
    "for byte_index in KEY_BYTE_RANGES:\n",
    "    # Get the obtained key byte and the correct key byte\n",
    "    obtained_byte = np.uint8(FOUND_KEY_BYTES[byte_index])\n",
    "    correct_byte  = np.uint8(CORRECT_KEY[byte_index])\n",
    "    \n",
    "    # Check if the entire byte matches\n",
    "    if obtained_byte == correct_byte:\n",
    "        correct_key_bytes += 1\n",
    "    \n",
    "    # Count the number of matching bits in the byte\n",
    "    print(f\"Byte {byte_index}: Obtained = {hex(obtained_byte)}, Correct = {hex(correct_byte)}\")\n",
    "    # Count the number of matching bits using XOR and bit counting\n",
    "    matching_bits = bin(~(obtained_byte ^ correct_byte)).count('1')\n",
    "    correct_key_bits += matching_bits\n",
    "\n",
    "# Print the results\n",
    "print(f\"Correct Key Bytes: {correct_key_bytes} / {len(KEY_BYTE_RANGES)}\")\n",
    "print(f\"Correct Key Bits: {correct_key_bits} / {len(KEY_BYTE_RANGES) * 8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
