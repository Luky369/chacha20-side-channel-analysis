{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-STM-SHIELDED\"\n",
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-STM\"\n",
    "FOLDER = \"ChaCha-100-000-Random-Nonce-STM-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-XMEGA\"\n",
    "# FOLDER = \"ChaCha-100-000-Random-Nonce-XMEGA-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPA_OUTPUT_FOLDER = \"_CPA_STM_2_S1_COL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6d931",
   "metadata": {},
   "source": [
    "# Load the content of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "NONCE_LEN_BYTES = 12\n",
    "TRACE_CNT = None\n",
    "TRACE_LEN = None\n",
    "TRACE_RANDOM_CNT = None\n",
    "CHUNK_SIZE = None\n",
    "CHUNKS_CNT = None\n",
    "LAST_CHUNK_SIZE = None\n",
    "def read_info(FOLDER):\n",
    "    \"\"\"\n",
    "    Read the info file to get the number of traces, chunk size, and trace length.\n",
    "    \"\"\"\n",
    "    global CHUNK_SIZE, LAST_CHUNK_SIZE\n",
    "    global CHUNKS_CNT\n",
    "    with open(f\"{FOLDER}/info.txt\", 'r') as file:\n",
    "        global TRACE_CNT\n",
    "        global TRACE_LEN\n",
    "        global TRACE_RANDOM_CNT\n",
    "        TRACE_CNT = int(file.readline())\n",
    "        TRACE_RANDOM_CNT = TRACE_CNT\n",
    "        CHUNK_SIZE = int(file.readline())\n",
    "        TRACE_LEN = int(file.readline())\n",
    "        \n",
    "    CHUNKS_CNT = math.ceil(TRACE_CNT / CHUNK_SIZE)\n",
    "    LAST_CHUNK_SIZE = TRACE_CNT - (CHUNKS_CNT - 1)* CHUNK_SIZE\n",
    "    print(f\"TRACE_CNT = {TRACE_CNT}\")   \n",
    "    print(f\"CHUNK_SIZE = {CHUNK_SIZE}\")   \n",
    "    print(f\"LAST_CHUNK_SIZE = {LAST_CHUNK_SIZE}\")   \n",
    "    print(f\"CHUNKS_CNT = {CHUNKS_CNT}\")   \n",
    "    print(f\"TRACE_RANDOM_CNT = {TRACE_RANDOM_CNT}\")   \n",
    "    print(f\"TRACE_LEN = {TRACE_LEN}\")\n",
    "    \n",
    "    \n",
    "read_info(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f88a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRACE_CNT = 500_000\n",
    "# CHUNKS_CNT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NONCES = None\n",
    "def read_nonces(FOLDER):\n",
    "    \"\"\"\n",
    "    Read the nonces from the binary files in the specified folder.\n",
    "    The nonces are stored in chunks, and the function concatenates them into a single array.\n",
    "    \"\"\"\n",
    "    global NONCES\n",
    "    global CHUNK_SIZE, LAST_CHUNK_SIZE\n",
    "    global CHUNKS_CNT\n",
    "    global TRACE_CNT\n",
    "    # Calculate the number of chunks\n",
    "    \n",
    "    nonces_list = []\n",
    "    for chunk_index in range(CHUNKS_CNT):\n",
    "        chunk_folder = os.path.join(FOLDER, f\"chunk_{chunk_index}\")\n",
    "        chunk_file = os.path.join(chunk_folder, \"nonces_random.bin\")\n",
    "        \n",
    "        if os.path.exists(chunk_file):\n",
    "            with open(chunk_file, 'rb') as file:\n",
    "                byte_array = file.read()\n",
    "            \n",
    "            if chunk_index != CHUNKS_CNT-1:\n",
    "                chunk = np.frombuffer(byte_array, dtype=np.uint8).reshape((CHUNK_SIZE, NONCE_LEN_BYTES))\n",
    "            else:\n",
    "                chunk = np.frombuffer(byte_array, dtype=np.uint8).reshape((LAST_CHUNK_SIZE, NONCE_LEN_BYTES))\n",
    "\n",
    "            nonces_list.append(chunk)\n",
    "        else:\n",
    "            print(f\"Chunk file {chunk_file} does not exist.\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    NONCES = np.vstack(nonces_list)\n",
    "        \n",
    "    # Desired shape\n",
    "    desired_shape = (TRACE_CNT, NONCE_LEN_BYTES)\n",
    "\n",
    "    # Check if the array has the desired shape\n",
    "    if NONCES.shape != desired_shape:\n",
    "        # Trim the array to the desired shape\n",
    "        print(f\"NONCES.shape != desired_shape - {NONCES.shape} != {desired_shape}\")\n",
    "        NONCES = NONCES[:desired_shape[0], :desired_shape[1]]\n",
    "        print(f\"Trimming NONCES to {NONCES.shape}\")\n",
    "        \n",
    "    nonce = NONCES[0]\n",
    "\n",
    "    # Convert each byte to its hexadecimal representation and join them\n",
    "    hex_representation = ''.join(f'{byte:02x}' for byte in nonce[:12])\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Nonce[0] = {hex_representation}\")\n",
    "    \n",
    "    \n",
    "read_nonces(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ca545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the correct key\n",
    "CORRECT_KEY = np.fromfile(f\"{FOLDER}/key.bin\", dtype=np.uint8)\n",
    "hex_key = ''.join(f'{byte:02x}' for byte in CORRECT_KEY)\n",
    "print(f\"Correct Key: {hex_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACES_RANDOM = None\n",
    "\n",
    "def load_traces_random(folder):\n",
    "    \"\"\"\n",
    "    Load traces from the specified folder. \n",
    "    The traces are stored in binary files (chunks, defined in info.txt file),\n",
    "    and the function concatenates them into a single array.\n",
    "    \"\"\"\n",
    "    global TRACES_RANDOM, CHUNK_SIZE, LAST_CHUNK_SIZE, CHUNKS_CNT, TRACE_CNT\n",
    "\n",
    "    traces_list = []\n",
    "    for chunk_index in range(CHUNKS_CNT):\n",
    "        chunk_folder = os.path.join(folder, f\"chunk_{chunk_index}\")\n",
    "        chunk_file = os.path.join(chunk_folder, \"traces_random.bin\")\n",
    "\n",
    "        if os.path.exists(chunk_file):\n",
    "            with open(chunk_file, 'rb') as file:\n",
    "                byte_array = file.read()\n",
    "\n",
    "            if chunk_index != CHUNKS_CNT-1:\n",
    "                chunk_traces = np.frombuffer(byte_array, dtype=np.uint16).reshape((CHUNK_SIZE, TRACE_LEN))\n",
    "            else:\n",
    "                chunk_traces = np.frombuffer(byte_array, dtype=np.uint16).reshape((LAST_CHUNK_SIZE, TRACE_LEN))\n",
    "\n",
    "            traces_list.append(chunk_traces)\n",
    "        else:\n",
    "            print(f\"Chunk file {chunk_file} does not exist.\")\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    TRACES_RANDOM = np.vstack(traces_list)\n",
    "\n",
    "load_traces_random(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdac7e9",
   "metadata": {},
   "source": [
    "# Adjust size of traces if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crop every trace (row) to sample range 0..1499\n",
    "# TRACE_CNT = 5000\n",
    "# TRACES_RANDOM = TRACES_RANDOM[:TRACE_CNT, :]\n",
    "# # TRACES_RANDOM = TRACES_RANDOM[:, 0:8_000]\n",
    "\n",
    "# NONCES= NONCES[:TRACE_CNT, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TRACES shape = {TRACES_RANDOM.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc466e08",
   "metadata": {},
   "source": [
    "# Helper functions and constants setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHACHA_KEY_LEN_BYTES = 32\n",
    "VALUES_IN_BYTE = 256 #0...255\n",
    "\n",
    "# constant sigma - 16B - \"expand 32-byte k\" transformed to bytes\n",
    "fixed_sigma = bytearray(\"expand 32-byte k\", \"utf-8\")\n",
    "print(hex(fixed_sigma[4]))\n",
    "\n",
    "# Hamming weight table for 8-bit values\n",
    "# This table is used to calculate the Hamming weight of a byte value.\n",
    "hamming_weight = [\n",
    "   0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, \n",
    "   2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2, 3, 2, 3, 3, 4, \n",
    "   2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, \n",
    "   5, 6, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, \n",
    "   3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, \n",
    "   4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, \n",
    "   1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, \n",
    "   4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, \n",
    "   4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, \n",
    "   3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, \n",
    "   4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, \n",
    "   6, 7, 4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8]\n",
    "\n",
    "def corr2_coeff(A, B):\n",
    "    \"\"\"\n",
    "    Calculate the correlation coefficient between two matrices A and B.\n",
    "    Each row of A and B is treated as a separate variable.\n",
    "    The function returns a matrix of correlation coefficients.\n",
    "    \"\"\"\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:, None]\n",
    "    B_mB = B - B.mean(1)[:, None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1)\n",
    "    ssB = (B_mB**2).sum(1)\n",
    "    \n",
    "        # Locate rows with zero sum of squares\n",
    "    zero_ssA_indices = np.where(ssA == 0)[0]\n",
    "    zero_ssB_indices = np.where(ssB == 0)[0]\n",
    "    \n",
    "    # Avoid division by zero by setting problematic rows to NaN\n",
    "    ssA[zero_ssA_indices] = np.nan\n",
    "    ssB[zero_ssB_indices] = np.nan\n",
    "    \n",
    "    one_part = np.sqrt(np.dot(ssA[:, None],ssB[None]))\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return np.dot(A_mA, B_mB.T) / one_part\n",
    "\n",
    "def leftRotate16(n):\n",
    "    return ((n << 16) | (n >> 16)) & 0xFFFFFFFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23f800",
   "metadata": {},
   "source": [
    "# Obtain the S[0], S[4], S[8], and S[12] based on already obtained K[4-15] and K[20-31]\n",
    "- potential only, when the recovery of all K[4-15] and K[20-31] bytes not successful\n",
    "- correct key used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import _ChaCha20_source\n",
    "\n",
    "# Reload the module to apply changes\n",
    "importlib.reload(_ChaCha20_source)\n",
    "\n",
    "# Import the updated functions\n",
    "from _ChaCha20_source import QUARTERROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global dictionary to store the correct state words\n",
    "# The words are the byte positions (0, 4, 8, 12), corresponding state words to state matrix\n",
    "CORRECT_STATE_1_COL_0 = {\n",
    "    0: None,\n",
    "    4: None,\n",
    "    8: None,\n",
    "    12: None\n",
    "}\n",
    "\n",
    "# Global dictionary to store the found state words\n",
    "FOUND_STATE_1_COL_0 = {\n",
    "    0: None,\n",
    "    4: None,\n",
    "    8: None,\n",
    "    12: None\n",
    "}\n",
    "\n",
    "# Global dictionary to store the calculated state words (S_1[x]) in column 1 - 3 (key bytes used to compute the columns 1 - 3)\n",
    "S1_COLUMNS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_s1_column_0():\n",
    "    \"\"\"\n",
    "    Calculate the first column of the ChaCha20 state using the correct key and fixed sigma.\n",
    "    This function uses the QUARTERROUND function to perform the necessary operations.\n",
    "    \"\"\"\n",
    "    global CORRECT_STATE_1_COL_0\n",
    "    col_0 = np.empty(4, dtype=np.uint32)  # 1 column - 4 words each (32-bit)\n",
    "\n",
    "    # Column 1: Fixed Sigma [4:7] Key[4:7] Key [20:23] Nonce [0:3]\n",
    "    col_0[0] = fixed_sigma[0] | (fixed_sigma[1] << 8) | (fixed_sigma[2] << 16) | (fixed_sigma[3] << 24)\n",
    "    col_0[1] = CORRECT_KEY[0]  | (CORRECT_KEY[1] << 8)  | (CORRECT_KEY[2] << 16)  | (CORRECT_KEY[3] << 24)\n",
    "    col_0[2] = CORRECT_KEY[16] | (CORRECT_KEY[17] << 8) | (CORRECT_KEY[18] << 16) | (CORRECT_KEY[19] << 24)\n",
    "    col_0[3] = 0x00_00_00_00\n",
    "\n",
    "    QUARTERROUND(col_0, 0, 1, 2, 3)\n",
    "    \n",
    "    CORRECT_STATE_1_COL_0[0]  = col_0[0]\n",
    "    CORRECT_STATE_1_COL_0[4]  = col_0[1]    \n",
    "    CORRECT_STATE_1_COL_0[8]  = col_0[2]    \n",
    "    CORRECT_STATE_1_COL_0[12] = col_0[3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a477d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_s1_column_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ce57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the correct state for each column\n",
    "print(\"Correct State 1 Column 0:\")\n",
    "for i in (0, 4, 8, 12):\n",
    "    if CORRECT_STATE_1_COL_0[i]:\n",
    "        print(f\"[{i:02}] {hex(CORRECT_STATE_1_COL_0[i])}\")\n",
    "    else:\n",
    "        print(f\"[{i:02}] None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_found_byte_into_s1_column_0(byte, word_index, byte_index):\n",
    "    \"\"\"\n",
    "    Updates the FOUND_STATE_1_COL_0 array by placing the given byte\n",
    "    into the correct position based on the word_index and byte_index.\n",
    "\n",
    "    Parameters:\n",
    "        byte (int): The byte value to be placed (0-255).\n",
    "        word_index (int): The index of the 32-bit word (0-3).\n",
    "        byte_index (int): The index of the byte within the 32-bit word (0-3).\n",
    "    \"\"\"\n",
    "    global FOUND_STATE_1_COL_0\n",
    "\n",
    "    # Ensure the byte is within the valid range\n",
    "    if not (0 <= byte <= 255):\n",
    "        raise ValueError(\"Byte must be in the range 0-255.\")\n",
    "    \n",
    "    # Ensure the word_index is within the valid range\n",
    "    if word_index not in (0, 4, 8, 12):\n",
    "        raise ValueError(\"Word index must be 0, 4, 8, or 12\")\n",
    "    \n",
    "    # Ensure the byte_index is within the valid range\n",
    "    if not (0 <= byte_index <= 3):\n",
    "        raise ValueError(\"Byte index must be in the range 0-3.\")\n",
    "\n",
    "    # Ensure FOUND_STATE_1_COL_0 is initialized\n",
    "    if FOUND_STATE_1_COL_0[word_index] is None:\n",
    "        FOUND_STATE_1_COL_0[word_index] = 0x00_00_00_00\n",
    "\n",
    "    # Clear the target byte position in the 32-bit word\n",
    "    FOUND_STATE_1_COL_0[word_index] &= ~(0xFF << (byte_index * 8))\n",
    "\n",
    "    # Insert the new byte into the correct position\n",
    "    FOUND_STATE_1_COL_0[word_index] |= (byte << (byte_index * 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial state of FOUND_STATE_1_COL_0\n",
    "print(\"Initial FOUND_STATE_1_COL_0:\")\n",
    "for i in (0, 4, 8, 12):\n",
    "    if FOUND_STATE_1_COL_0[i]:\n",
    "        print(f\"[{i}] {hex(FOUND_STATE_1_COL_0[i])}\")\n",
    "    else:\n",
    "        print(f\"[{i}] None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_s1_columns_1_to_3(key_bytes_array):\n",
    "    \"\"\"\n",
    "    Calculate the S1_COLUMNS for every trace.\n",
    "    S1_COLUMNS will have the shape (TRACE_CNT, 3, 4), where each row contains\n",
    "    the 3 columns of the ChaCha20 state (excluding the first constant column).\n",
    "\n",
    "    Parameters:\n",
    "        key_bytes_array (np.ndarray): Array of key bytes (shape: [32]).\n",
    "    \"\"\"\n",
    "    global S1_COLUMNS\n",
    "    S1_COLUMNS = np.empty((TRACE_CNT, 3, 4), dtype=np.uint32)  # 3 columns, 4 words each (32-bit)\n",
    "\n",
    "    # Iterate over each trace\n",
    "    for trace_idx in range(TRACE_CNT):\n",
    "        # Extract the nonce for the current trace\n",
    "        nonce = NONCES[trace_idx]\n",
    "\n",
    "        # Column 1: Fixed Sigma [4:7] Key[4:7] Key [20:23] Nonce [0:3]\n",
    "        S1_COLUMNS[trace_idx][0] = (\n",
    "            (fixed_sigma[4] | (fixed_sigma[5] << 8) | (fixed_sigma[6] << 16) | (fixed_sigma[7] << 24)),\n",
    "            (key_bytes_array[4] | (key_bytes_array[5] << 8) | (key_bytes_array[6] << 16) | (key_bytes_array[7] << 24)),\n",
    "            (key_bytes_array[20] | (key_bytes_array[21] << 8) | (key_bytes_array[22] << 16) | (key_bytes_array[23] << 24)),\n",
    "            (nonce[0] | (nonce[1] << 8) | (nonce[2] << 16) | (nonce[3] << 24))\n",
    "        )\n",
    "        # Column 1: Fixed Sigma [8:11] Key[8:11] Key [24:27] Nonce [4:7]\n",
    "        S1_COLUMNS[trace_idx][1] = (\n",
    "            (fixed_sigma[8] | (fixed_sigma[9] << 8) | (fixed_sigma[10] << 16) | (fixed_sigma[11] << 24)),\n",
    "            (key_bytes_array[8] | (key_bytes_array[9] << 8) | (key_bytes_array[10] << 16) | (key_bytes_array[11] << 24)),\n",
    "            (key_bytes_array[24] | (key_bytes_array[25] << 8) | (key_bytes_array[26] << 16) | (key_bytes_array[27] << 24)),\n",
    "            (nonce[4] | (nonce[5] << 8) | (nonce[6] << 16) | (nonce[7] << 24))\n",
    "        )\n",
    "\n",
    "        # Column 1: Fixed Sigma [12:15] Key[12:15] Key [28:31] Nonce [8:11]\n",
    "        S1_COLUMNS[trace_idx][2] = (\n",
    "            (fixed_sigma[12] | (fixed_sigma[13] << 8) | (fixed_sigma[14] << 16) | (fixed_sigma[15] << 24)),\n",
    "            (key_bytes_array[12] | (key_bytes_array[13] << 8) | (key_bytes_array[14] << 16) | (key_bytes_array[15] << 24)),\n",
    "            (key_bytes_array[28] | (key_bytes_array[29] << 8) | (key_bytes_array[30] << 16) | (key_bytes_array[31] << 24)),\n",
    "            (nonce[8] | (nonce[9] << 8) | (nonce[10] << 16) | (nonce[11] << 24))\n",
    "        )\n",
    "\n",
    "        QUARTERROUND(S1_COLUMNS[trace_idx][0], 0, 1, 2, 3)\n",
    "        QUARTERROUND(S1_COLUMNS[trace_idx][1], 0, 1, 2, 3)\n",
    "        QUARTERROUND(S1_COLUMNS[trace_idx][2], 0, 1, 2, 3)\n",
    "\n",
    "\n",
    "    print(f\"S1_COLUMNS calculated with shape: {S1_COLUMNS.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e969443",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_s1_columns_1_to_3(CORRECT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9789a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming S1_COLUMNS is a NumPy array with shape (TRACE_CNT, 3, 4)\n",
    "# for trace_idx in range(S1_COLUMNS.shape[0]):  # Iterate over traces\n",
    "#     print(f\"Trace {trace_idx}:\")\n",
    "#     for col_idx in range(S1_COLUMNS.shape[1]):  # Iterate over columns\n",
    "#         column = S1_COLUMNS[trace_idx][col_idx]\n",
    "#         hex_values = [hex(value) for value in column]  # Convert each value to hex\n",
    "#         print(f\"  Column {col_idx}: {hex_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678dbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_row_of_intermediate_values_for_s1_0_4_12(s1_columns,\n",
    "                                                       s1_index,\n",
    "                                                       byte_index,\n",
    "                                                       obtained_S1_col_0=None):\n",
    "    \"\"\"\n",
    "    Calculate one row of intermediate values for a specific S1 byte (0, 4, or 12) \n",
    "    based on the S1 columns and fixed sigma.\n",
    "\n",
    "    Parameters:\n",
    "        s1_columns (np.ndarray): The S1 columns 1-3 (shape: [3, 4]). (column 1 = [0], column 2 = [1], column 3 = [2])\n",
    "        s1_index (int): The index of the S1 byte (0, 4, or 12).\n",
    "        byte_index (int): The byte index within the word (0-3).\n",
    "        obtained_S1_col_0 (dict): A dictionary containing the obtained S1 column 0 values (0, 4, 8, 12).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Intermediate values for all possible S1 byte values (shape: [256]).\n",
    "    \"\"\"\n",
    "    global hamming_weight\n",
    "    \n",
    "    if obtained_S1_col_0 is None:\n",
    "        obtained_S1_col_0  = {0:0 , 4:0 , 8:0 , 12:0} # 4 bytes - 32 bits\n",
    "\n",
    "    Sx_word = None\n",
    "    Sy_byte = None\n",
    "    Sx_byte_prev = None\n",
    "\n",
    "    Sy_word = None\n",
    "    Sy_byte = None\n",
    "    Sy_byte_prev = None\n",
    "\n",
    "    Sz_word = None\n",
    "    Sz_byte = None\n",
    "\n",
    "    # Extract the S1 columns for the specified index\n",
    "    # For S1_8 separate function, since leakage function is different\n",
    "    if s1_index == 0:    # Sx = S1_0 - target, Sy = S1_5,          Sz = S1_15 \n",
    "        Sx_word = obtained_S1_col_0[0]\n",
    "        Sy_word = s1_columns[0][1]\n",
    "        Sz_word = s1_columns[2][3]\n",
    "    elif s1_index == 4:  # Sx = S1_3,          Sy = S1_4 - target, Sz = S1_14 \n",
    "        Sx_word = s1_columns[2][0]\n",
    "        Sy_word = obtained_S1_col_0[4]\n",
    "        Sz_word = s1_columns[1][3]\n",
    "    elif s1_index == 12: # Sx = S1_2,          Sy = S1_6,          Sz = S1_12 - target\n",
    "        Sx_word = s1_columns[0][0]\n",
    "        Sy_word = s1_columns[1][1]\n",
    "        Sz_word = obtained_S1_col_0[12]\n",
    "    else:\n",
    "        raise ValueError(\"s1_index must be one of 0, 4, or 12.\")\n",
    "\n",
    "    carry = 0\n",
    "    # Determine if carry is needed\n",
    "    if byte_index % 4 != 0:\n",
    "        # Extract the previous byte from the 32-bit word\n",
    "        Sx_byte_prev = Sx_word >> (8 * (byte_index - 1)) & 0xFF\n",
    "        Sy_byte_prev = Sy_word >> (8 * (byte_index - 1)) & 0xFF\n",
    "\n",
    "        sum_bytes = np.uint16(Sx_byte_prev) + np.uint16(Sy_byte_prev)\n",
    "        carry = 1 if (sum_bytes) > 0xFF else 0\n",
    "\n",
    "    # Extract the current bytes\n",
    "    Sx_byte = Sx_word >> (8 * byte_index) & 0xFF\n",
    "    Sy_byte = Sy_word >> (8 * byte_index) & 0xFF\n",
    "    Sz_byte = Sz_word >> (8 * byte_index) & 0xFF\n",
    "    \n",
    "    # Calculate possible values based on the leakage function\n",
    "    possible_values_vector = None\n",
    "\n",
    "    if s1_index == 0: \n",
    "        possible_values_vector = np.array([\n",
    "        Sz_byte ^ ((candidate + Sy_byte + carry) & 0xFF)\n",
    "        for candidate in range(256)\n",
    "    ], dtype=np.uint8)\n",
    "    elif s1_index == 4:\n",
    "        possible_values_vector = np.array([\n",
    "        Sz_byte ^ ((Sx_byte + candidate + carry) & 0xFF)\n",
    "        for candidate in range(256)\n",
    "    ], dtype=np.uint8)\n",
    "    elif s1_index == 12:\n",
    "        possible_values_vector = np.array([\n",
    "        candidate ^ ((Sx_byte + Sy_byte + carry) & 0xFF)\n",
    "        for candidate in range(256)\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    # Convert to hamming weights (or other leakage model)\n",
    "    intermediate_values = np.array([hamming_weight[value] for value in possible_values_vector], dtype=np.uint16)\n",
    "   \n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rotated_previous_intermediate_values_from_s1():\n",
    "    \"\"\"\n",
    "    Calculate the rotated previous intermediate values from S1 columns 1, 2, and 3. (needed form carry bit calculation)\n",
    "    This function uses the S1_COLUMNS to compute the intermediate values.\n",
    "    \"\"\"\n",
    "    global S1_COLUMNS\n",
    "\n",
    "    intermediate_values = np.empty(TRACE_CNT, dtype=np.uint32)  # Store 32-bit intermediate values\n",
    "\n",
    "    for i in range(TRACE_CNT):\n",
    "        # (((S_1[2]_i + S_1[7]_i) XOR S_1[13]_i) <<< 16)) \n",
    "        intermediate_values[i] = S1_COLUMNS[i][0][3] ^ (S1_COLUMNS[i][1][0] + S1_COLUMNS[i][2][1])\n",
    "        #Rotate the intermediate value left by 16 bits\n",
    "        intermediate_values[i] = leftRotate16(intermediate_values[i])\n",
    "\n",
    "    # Convert the intermediate values to numpy array of 4 x uint8\n",
    "    intermediate_values = np.array(\n",
    "    [\n",
    "        (\n",
    "            (intermediate_values[i]         & 0xFF),\n",
    "            ((intermediate_values[i] >>  8) & 0xFF),\n",
    "            ((intermediate_values[i] >> 16) & 0xFF),\n",
    "            ((intermediate_values[i] >> 24) & 0xFF)\n",
    "        ) for i in range(TRACE_CNT)\n",
    "    ], dtype=np.uint8)\n",
    "    \n",
    "    #Print first intermediate value in HEX after conversion (4 bytes)\n",
    "    print(f\"Intermediate value[0] = {hex(intermediate_values[0][0])} {hex(intermediate_values[0][1])} {hex(intermediate_values[0][2])} {hex(intermediate_values[0][3])}\")\n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643387f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_row_of_intermediate_values_for_s1_8(s1_columns, \n",
    "                                                  byte_index, \n",
    "                                                  prev_intermediate_values, \n",
    "                                                  obtained_S1_col_0 = None,\n",
    "                                                  include_XOR = True):\n",
    "    \"\"\"\n",
    "    Calculate one row of intermediate values for a specific S1 byte (16-19) based on the S1 columns and fixed sigma.    \n",
    "\n",
    "    Parameters:\n",
    "        s1_columns (np.ndarray): The S1 columns (shape: [3, 4]).\n",
    "        byte_index (int): Index of the S1 byte (16-19).\n",
    "        prev_intermediate_values (np.ndarray): The previous intermediate values (shape: [4]).\n",
    "        obtained_S1_col_0 (dict): A dictionary containing the obtained S1 column 0 values (0, 4, 8, 12).\n",
    "        include_XOR (bool): If True, include XOR operation in the calculation.\n",
    "    Returns:\n",
    "        np.ndarray: Intermediate values for all possible S1 byte values (shape: [256]).\n",
    "    \"\"\"\n",
    "    global S1_COLUMNS, CORRECT_KEY, hamming_weight\n",
    "\n",
    "    carry = 0\n",
    "\n",
    "    if 0 <= byte_index <= 3:\n",
    "\n",
    "        s1_8_word = obtained_S1_col_0[8]\n",
    "        s1_7_word = s1_columns[2][1]\n",
    "        \n",
    "        # Determine if carry is needed\n",
    "        if byte_index != 0:\n",
    "            if s1_8_word is None:\n",
    "                raise ValueError(\"s1_8_word must be provided for carry calculation.\")\n",
    "            # extract the byte from the 32-bit word - (byte_index - 1)th byte\n",
    "            prev_intermediate_value_byte_prev = prev_intermediate_values[byte_index - 1]\n",
    "\n",
    "            s1_8_byte_prev = s1_8_word >> (8 * (byte_index - 1)) & 0xFF\n",
    "            sum_bytes = s1_8_byte_prev + prev_intermediate_value_byte_prev.astype(np.uint16)\n",
    "            carry = 1 if (sum_bytes) > 0xFF else 0\n",
    "\n",
    "        s1_7_byte = s1_7_word >> (8 * (byte_index)) & 0xFF\n",
    "        prev_intermediate_value_byte = prev_intermediate_values[byte_index]\n",
    "        # Calculate possible values based on the nonce byte and fixed sigma\n",
    "\n",
    "        possible_values_vector = None\n",
    "        if include_XOR:   \n",
    "            possible_values_vector = np.array([\n",
    "                s1_7_byte ^ ((s1_byte_candidate + prev_intermediate_value_byte + carry) & 0xFF)\n",
    "                for s1_byte_candidate in range(256)\n",
    "            ], dtype=np.uint8)\n",
    "        else:\n",
    "            possible_values_vector = np.array([\n",
    "                ((s1_byte_candidate + prev_intermediate_value_byte + carry) & 0xFF)\n",
    "                for s1_byte_candidate in range(256)\n",
    "            ], dtype=np.uint8)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Byte index must be in the range 0-3.\")\n",
    "\n",
    "    # Convert to hamming weights (or other leakage model)\n",
    "    intermediate_values = np.array([hamming_weight[value] for value in possible_values_vector], dtype=np.uint16)\n",
    "\n",
    "    return intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_matrix_s1(s1_word_index,\n",
    "                                    byte_index, \n",
    "                                    num_traces, \n",
    "                                    include_XOR,\n",
    "                                    trace_shift=0, \n",
    "                                    obtained_s1_col_0 = CORRECT_STATE_1_COL_0,\n",
    "                                    previous_intermediate_values = None):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix for a specific byte index based on the S1 columns and traces.\n",
    "\n",
    "    Parameters:\n",
    "        s1_col0_word_index (int): Index of the S1 word (0, 4, 8, 12).\n",
    "        byte_index (int): Index of the S1 byte (0-3)\n",
    "        num_traces (int): Number of traces to use for the calculation.\n",
    "        include_XOR (bool): If True, include XOR operation in the calculation for S1_8.  \n",
    "        trace_shift (int): Number of traces to shift for the calculation. Default is 0.\n",
    "        obtained_s1_col_0 (np.ndarray): The obtained S1 byte from column 0 (shape: [4]).\n",
    "        previous_intermediate_values (np.ndarray): The previous intermediate values (shape: [4]).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The correlation matrix (shape: [num_traces, 256]).\n",
    "    \"\"\"\n",
    "    global TRACES_RANDOM, S1_COLUMNS, CORRECT_KEY, NONCES, CORRECT_STATE_1_COL_0, FOUND_STATE_1_COL_0\n",
    "    global hamming_weight\n",
    "\n",
    "    # Ensure the number of traces does not exceed available traces\n",
    "    max_traces = TRACES_RANDOM.shape[0]\n",
    "    if trace_shift >= max_traces:\n",
    "        raise ValueError(f\"trace_shift ({trace_shift}) exceeds the number of available traces ({max_traces}).\")\n",
    "    \n",
    "    num_traces = min(num_traces, max_traces - trace_shift)\n",
    "\n",
    "    # Slice the traces and nonces to the specified number of traces\n",
    "    traces = TRACES_RANDOM[trace_shift:num_traces + trace_shift, :]\n",
    "    s1_columns = S1_COLUMNS[trace_shift:num_traces + trace_shift, :]\n",
    "\n",
    "    # Placeholder for intermediate values (to be calculated later)\n",
    "    L_matrix = np.empty((num_traces, 256), dtype=np.uint16) #TODO: unify?\n",
    "\n",
    "    # Check if previous intermediate values are needed and if calculated\n",
    "    if s1_word_index == 8 and previous_intermediate_values is None:\n",
    "        raise ValueError(\"Previous intermediate values are required for S1_8 word.\")\n",
    "    \n",
    "    if  byte_index not in range(0, 4):\n",
    "        raise ValueError(\"Byte index must be in the range 0-3.\")\n",
    "    \n",
    "    # Calculate intermediate values for each trace\n",
    "    for trace_idx in range(num_traces):\n",
    "        if s1_word_index == 0 or s1_word_index == 4 or s1_word_index == 12:\n",
    "            L_matrix[trace_idx, :] = calculate_row_of_intermediate_values_for_s1_0_4_12(\n",
    "                s1_columns[trace_idx],\n",
    "                s1_word_index,\n",
    "                byte_index, \n",
    "                obtained_s1_col_0\n",
    "            )\n",
    "        elif s1_word_index == 8:\n",
    "            L_matrix[trace_idx, :] = calculate_row_of_intermediate_values_for_s1_8(\n",
    "                s1_columns[trace_idx],\n",
    "                byte_index,\n",
    "                previous_intermediate_values[trace_idx + trace_shift],\n",
    "                obtained_s1_col_0,\n",
    "                include_XOR\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Byte index must be in the correct range\")\n",
    "\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = corr2_coeff(L_matrix.T, traces.T)\n",
    "\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6933bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_matrix_for_s1_words(\n",
    "    s1_word_indices, \n",
    "    byte_indices, \n",
    "    num_traces,\n",
    "    include_XOR,\n",
    "    trace_shift=0, \n",
    "    obtained_s1_col_0=CORRECT_STATE_1_COL_0, \n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix for specified S1 word indices and byte indices.\n",
    "\n",
    "    Parameters:\n",
    "        s1_word_indices (list): A list of S1 word indices (e.g., [0, 4, 8, 12]).\n",
    "        byte_indices (list): A list of byte indices (0-3) for each word.\n",
    "        num_traces (int): Number of traces to use for the calculation.\n",
    "        include_XOR (bool): If True, include XOR operation in the calculation for S1_8.\n",
    "        trace_shift (int): Number of traces to shift for the calculation. Default is 0.\n",
    "        obtained_s1_col_0 (np.ndarray): Array of obtained S1 column 0 bytes (shape: [4]).\n",
    "    \"\"\"\n",
    "    global CORRELATION_MATRIXES_S1, PREVIOUS_INTERMEDIATE_VALUES_S1\n",
    "\n",
    "    for s1_word_index in s1_word_indices:\n",
    "        for byte_index in byte_indices:\n",
    "            print(f\"Calculating correlation matrix for S1 word {s1_word_index}, byte {byte_index}...\")\n",
    "\n",
    "            if s1_word_index in [0, 4, 12]:\n",
    "                # Use the function for S1 words 0, 4, and 12\n",
    "                CORRELATION_MATRIXES_S1[(s1_word_index, byte_index)] = calculate_correlation_matrix_s1(\n",
    "                    s1_word_index=s1_word_index,\n",
    "                    byte_index=byte_index,\n",
    "                    num_traces=num_traces,\n",
    "                    include_XOR=include_XOR,\n",
    "                    trace_shift=trace_shift,\n",
    "                    obtained_s1_col_0=obtained_s1_col_0\n",
    "                )\n",
    "            elif s1_word_index == 8:\n",
    "                # Use the function for S1 word 8, which requires previous intermediate values\n",
    "                if PREVIOUS_INTERMEDIATE_VALUES_S1 is None:\n",
    "                    raise ValueError(\"Previous intermediate values are required for S1 word 8.\")\n",
    "                \n",
    "                CORRELATION_MATRIXES_S1[(s1_word_index, byte_index)] = calculate_correlation_matrix_s1(\n",
    "                    s1_word_index=s1_word_index,\n",
    "                    byte_index=byte_index,\n",
    "                    num_traces=num_traces,\n",
    "                    include_XOR=include_XOR,\n",
    "                    trace_shift=trace_shift,\n",
    "                    obtained_s1_col_0=obtained_s1_col_0,\n",
    "                    previous_intermediate_values=PREVIOUS_INTERMEDIATE_VALUES_S1\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping unsupported S1 word {s1_word_index}.\")\n",
    "\n",
    "    print(\"Correlation matrices calculation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0dfe3",
   "metadata": {},
   "source": [
    "# Correlation matrixes processing (Plot, (Sub)Key retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f67ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of traces to use for the calculation\n",
    "# This can be adjusted based on the available data and requirements\n",
    "TRACES_USED = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ad344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrices for S1 words 0, 4, 8, and 12 (byte indices 0-3)\n",
    "CORRELATION_MATRIXES_S1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the previous intermediate values, needed for S1_8 acquisition\n",
    "# Previous intermediate values are calculated from S1 columns 1, 2, and 3 (previous knowledge of key bytes is needed. Simulated using the correct key)\n",
    "# The function calculates the intermediate values for all traces and stores them in a global variable.\n",
    "PREVIOUS_INTERMEDIATE_VALUES_S1 = calculate_rotated_previous_intermediate_values_from_s1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the S1 byte values\n",
    "CORRECT_STATE_1_COL_0_BYTES = {\n",
    "    (0, 0):  (CORRECT_STATE_1_COL_0[0] & 0xFF),\n",
    "    (0, 1): ((CORRECT_STATE_1_COL_0[0] >> 8) & 0xFF),\n",
    "    (0, 2): ((CORRECT_STATE_1_COL_0[0] >> 16) & 0xFF),\n",
    "    (0, 3): ((CORRECT_STATE_1_COL_0[0] >> 24) & 0xFF),\n",
    "    \n",
    "    (4, 0):  (CORRECT_STATE_1_COL_0[4] & 0xFF),\n",
    "    (4, 1): ((CORRECT_STATE_1_COL_0[4] >> 8) & 0xFF),\n",
    "    (4, 2): ((CORRECT_STATE_1_COL_0[4] >> 16) & 0xFF),\n",
    "    (4, 3): ((CORRECT_STATE_1_COL_0[4] >> 24) & 0xFF),\n",
    "    \n",
    "    (8, 0):  (CORRECT_STATE_1_COL_0[8] & 0xFF),\n",
    "    (8, 1): ((CORRECT_STATE_1_COL_0[8] >> 8) & 0xFF),\n",
    "    (8, 2): ((CORRECT_STATE_1_COL_0[8] >> 16) & 0xFF),\n",
    "    (8, 3): ((CORRECT_STATE_1_COL_0[8] >> 24) & 0xFF),\n",
    "    \n",
    "    (12, 0):  (CORRECT_STATE_1_COL_0[12] & 0xFF),\n",
    "    (12, 1): ((CORRECT_STATE_1_COL_0[12] >> 8) & 0xFF),\n",
    "    (12, 2): ((CORRECT_STATE_1_COL_0[12] >> 16) & 0xFF),\n",
    "    (12, 3): ((CORRECT_STATE_1_COL_0[12] >> 24) & 0xFF),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the dictionary and print the hex values\n",
    "for (word_index, byte_index), byte_value in CORRECT_STATE_1_COL_0_BYTES.items():\n",
    "    print(f\"S1[{word_index}, {byte_index}] = {hex(byte_value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_correlation_matrix_s1(\n",
    "    s1_word_index,\n",
    "    byte_index, \n",
    "    show_plots=True, \n",
    "    save_plots=False, \n",
    "    zoom_plot=False,\n",
    "    list_correlations=False,\n",
    "    correct_s1_bytes=CORRECT_STATE_1_COL_0_BYTES,\n",
    "    output_folder=\"plots\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the correlation matrix for a given key byte index, optionally display and save the plots,\n",
    "    and save the found key byte value to a global array.\n",
    "\n",
    "    Parameters:\n",
    "        s1_word_index (int): The S1 word index being processed (0, 4, 8, or 12).\n",
    "        byte_index (int): The key byte index being processed. (0-3)\n",
    "        show_plots (bool): Whether to display the plot.\n",
    "        save_plots (bool): Whether to save the plot as an image.\n",
    "        zoom_plot (bool): Whether to zoom into the range defined by `byte_ranges`.\n",
    "        list_correlations (bool): Whether to list the correlations for all key candidates.\n",
    "        correct_s1_bytes (np.ndarray): The correct S1 bytes in 4 x 4 matrix\n",
    "        output_folder (str): Folder to save the plots if `save_plots` is True.\n",
    "    \"\"\"\n",
    "    global FOUND_STATE_1_COL_0, CORRELATION_MATRIXES_S1, BYTE_RANGES_S1, VALUES_IN_BYTE\n",
    "\n",
    "    if byte_index < 0 or byte_index > 3:\n",
    "        raise ValueError(\"Byte index must be in the range 0-3.\")\n",
    "    \n",
    "    if s1_word_index not in [0, 4, 8, 12]:\n",
    "        raise ValueError(\"S1 word index must be one of 0, 4, 8, or 12.\")\n",
    "\n",
    "    # Determine the correct key byte value for the given byte index\n",
    "    correct_s1_byte = correct_s1_bytes[(s1_word_index,byte_index)]\n",
    "\n",
    "    # Get the correlation matrix cut-out for the current byte index\n",
    "    correlation_matrix_cutout = CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][:, BYTE_RANGES_S1[(s1_word_index,byte_index)][0]:BYTE_RANGES_S1[(s1_word_index,byte_index)][1]]\n",
    "\n",
    "    # Find the minimum correlation value and its corresponding key candidate\n",
    "    corr_min = np.unravel_index(correlation_matrix_cutout.argmin(), correlation_matrix_cutout.shape)\n",
    "    obtained_s1_min = corr_min[0]\n",
    "    min_value = correlation_matrix_cutout[corr_min]\n",
    "\n",
    "    insert_found_byte_into_s1_column_0(obtained_s1_min, s1_word_index, byte_index)\n",
    "    # Find the correlation of the correct key byte\n",
    "    correct_s1_correlation = np.min(correlation_matrix_cutout[correct_s1_byte])\n",
    "\n",
    "    output = f\"$S_1[{s1_word_index}][{byte_index}]$ & {BYTE_RANGES_S1[(s1_word_index,byte_index)]} & \\\\texttt{{{format(obtained_s1_min, '#04X').replace('0X', '0x')}}} ({min_value:.5f}) & \\\\texttt{{{format(correct_s1_byte, '#04X').replace('0X', '0x')}}} ({correct_s1_correlation:.5f})\"\n",
    "    print(output)\n",
    "\n",
    "    if list_correlations:\n",
    "        # Calculate the minimum correlation value for each key candidate\n",
    "        min_correlations = []\n",
    "        for candidate in range(VALUES_IN_BYTE):\n",
    "            min_corr = np.min(correlation_matrix_cutout[candidate])\n",
    "            min_correlations.append((min_corr, candidate))\n",
    "        \n",
    "        # Sort the candidates based on their minimum correlation values\n",
    "        min_correlations.sort()  # Sort by the first element of the tuple (min_corr)\n",
    "        \n",
    "        # Print the top 10 sorted candidates and their correlation values\n",
    "        print(f\"Top 10 sorted candidates for S1 byte {byte_index}:\")\n",
    "        for i in range(min(10, len(min_correlations))):  # Ensure we don't exceed the list length\n",
    "            print(f\"Candidate: {hex(min_correlations[i][1])}, Correlation: {min_correlations[i][0]}\")\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)  # Ensure the folder exists\n",
    "        output_file = os.path.join(output_folder, f\"correlation_results_s1_{s1_word_index}_{byte_index}.txt\")\n",
    "\n",
    "        with open(output_file, \"w\") as file:\n",
    "            for i in range(0, len(min_correlations)):\n",
    "                file.write(f\"Candidate: {hex(min_correlations[i][1])}, Correlation: {min_correlations[i][0]}\\n\")\n",
    "        \n",
    "    if not show_plots and not save_plots:\n",
    "        return\n",
    "\n",
    "    # Prepare the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plot all key candidates\n",
    "    for corr_i in range(VALUES_IN_BYTE):\n",
    "        if corr_i != correct_s1_byte and corr_i != obtained_s1_min:\n",
    "            plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][corr_i], 'k.', alpha=0.3)\n",
    "\n",
    "    # Highlight the correct key byte and the guessed key byte\n",
    "    plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][obtained_s1_min], 'r.', label=f\"Guessed byte = {hex(obtained_s1_min)}\")\n",
    "    plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][correct_s1_byte], 'g.', label=f\"Correct byte = {hex(correct_s1_byte)}\")\n",
    "\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel(\"Trace Sample\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    start_col, end_col = BYTE_RANGES_S1[(s1_word_index,byte_index)]\n",
    "    plt.axvline(x=start_col, color='b', linestyle='--', label='Search Range')\n",
    "    plt.axvline(x=end_col, color='b', linestyle='--')\n",
    "    \n",
    "    # Save the plot if required\n",
    "    if save_plots:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        plot_path = os.path.join(output_folder, f\"s1_{s1_word_index:02}_byte_{byte_index:02}.png\")\n",
    "        plt.savefig(plot_path, bbox_inches='tight', dpi=600)\n",
    "        print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "    # Show the plot\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Add zoomed range if enabled\n",
    "    if zoom_plot:\n",
    "        # Prepare the plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Plot all key candidates\n",
    "        for corr_i in range(VALUES_IN_BYTE):\n",
    "            if corr_i != correct_s1_byte and corr_i != obtained_s1_min:\n",
    "                plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][corr_i], 'k.', alpha=0.3)\n",
    "\n",
    "        # Highlight the correct key byte and the guessed key byte\n",
    "        plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][obtained_s1_min], 'r.', label=f\"Guessed byte = {hex(obtained_s1_min)}\")\n",
    "        plt.plot(CORRELATION_MATRIXES_S1[(s1_word_index,byte_index)][correct_s1_byte], 'g.', label=f\"Correct byte = {hex(correct_s1_byte)}\")\n",
    "\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.xlabel(\"Trace Sample\")\n",
    "        plt.ylabel(\"Correlation\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.xlim(start_col, end_col)\n",
    "\n",
    "        # Show the plot\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "\n",
    "        # Save the plot if required\n",
    "        if save_plots:\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            plot_path = os.path.join(output_folder, f\"s1_{s1_word_index:02}_byte_{byte_index:02}_zoomed.png\")\n",
    "            plt.savefig(plot_path, bbox_inches='tight', dpi=600)\n",
    "            print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "        # Close the plot to free memory\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67559833",
   "metadata": {},
   "source": [
    "## Execution of the anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe396b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_XOR = \"STM\" in FOLDER\n",
    "print(f\"INCLUDE_XOR = {INCLUDE_XOR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the specified byte range\n",
    "# This will calculate the correlation matrix for state 1 words (0, 4, 8, 12) and bytes (0, 1, 2, 3)\n",
    "# and store them in the CORRELATION_MATRIXES_S1 dictionary.\n",
    "calculate_correlation_matrix_for_s1_words(\n",
    "    s1_word_indices=[0, 4, 8, 12],\n",
    "    byte_indices=[0, 1, 2, 3],\n",
    "    num_traces=TRACES_USED,\n",
    "    include_XOR=INCLUDE_XOR,\n",
    "    trace_shift=0, \n",
    "    obtained_s1_col_0=CORRECT_STATE_1_COL_0\n",
    ")\n",
    "# calculate_correlation_matrix_for_s1_words(\n",
    "#     s1_word_indices=[8],\n",
    "#     byte_indices=[1],\n",
    "#     num_traces=TRACES_USED, \n",
    "#     include_XOR=INCLUDE_XOR,\n",
    "#     trace_shift=0, \n",
    "#     obtained_s1_col_0=CORRECT_STATE_1_COL_0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3467620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the byte ranges for each s1 word and byte index within the S_1 column 0.\n",
    "# The ranges are defined based on the analysis of the correlation matrices.\n",
    "# The ranges are adjusted to focus on the most relevant samples for each key byte.\n",
    "# The ranges are defined as (start, end) tuples, where start and end are the sample indices.\n",
    "BYTE_RANGES_S1_STM = {\n",
    "  (0,0): (0, TRACE_LEN), # S1_0\n",
    "  (0,1): (0, TRACE_LEN),\n",
    "  (0,2): (0, TRACE_LEN),\n",
    "  (0,3): (0, TRACE_LEN),\n",
    "  (4,0): (0, TRACE_LEN), # S1_4\n",
    "  (4,1): (0, TRACE_LEN),\n",
    "  (4,2): (0, TRACE_LEN),\n",
    "  (4,3): (0, TRACE_LEN),\n",
    "  (8,0): (0, TRACE_LEN), # S1_8\n",
    "  (8,1): (0, TRACE_LEN),\n",
    "  (8,2): (0, TRACE_LEN),\n",
    "  (8,3): (0, TRACE_LEN),\n",
    "  (12,0): (0, TRACE_LEN), # S1_12\n",
    "  (12,1): (0, TRACE_LEN),\n",
    "  (12,2): (0, TRACE_LEN),\n",
    "  (12,3): (0, TRACE_LEN),\n",
    "}\n",
    "\n",
    "BYTE_RANGES_S1 = BYTE_RANGES_S1_STM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f95899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the byte ranges for each s1 word and byte index within the S_1 column 0.\n",
    "# # The ranges are defined based on the analysis of the correlation matrices.\n",
    "# # The ranges are adjusted to focus on the most relevant samples for each key byte.\n",
    "# # The ranges are defined as (start, end) tuples, where start and end are the sample indices.\n",
    "# BYTE_RANGES_S1_XMEGA = {\n",
    "#   (0,0):  (11500, 13000), # S1_0\n",
    "#   (0,1):  (11500, 13000),\n",
    "#   (0,2):  (11500, 13000),\n",
    "#   (0,3):  (11500, 13000),\n",
    "#   (4,0):  (18500, 20500), # S1_4\n",
    "#   (4,1):  (18500, 20500),\n",
    "#   (4,2):  (18500, 20500),\n",
    "#   (4,3):  (18500, 20500),\n",
    "#   (8,0):  (16500, 17500), # S1_8\n",
    "#   (8,1):  (16500, 17500),\n",
    "#   (8,2):  (16500, 17500),\n",
    "#   (8,3):  (16500, 17500),\n",
    "#   (12,0): (14000, 15500), # S1_12\n",
    "#   (12,1): (14000, 15500),\n",
    "#   (12,2): (14500, 15500),\n",
    "#   (12,3): (14000, 15500),\n",
    "# }\n",
    "\n",
    "# BYTE_RANGES_S1 = BYTE_RANGES_S1_XMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5ee81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the specified word indices (0, 4, 8, 12)\n",
    "for word_index in [0, 4, 8, 12]:\n",
    "    # Iterate over the byte indices (0-3)\n",
    "    for byte_index in range(4):\n",
    "        # Call the function for each combination of word and byte index\n",
    "        process_and_plot_correlation_matrix_s1(\n",
    "            s1_word_index=word_index,\n",
    "            byte_index=byte_index,\n",
    "            show_plots=False,\n",
    "            save_plots=False,\n",
    "            zoom_plot=False,\n",
    "            list_correlations=False,\n",
    "            correct_s1_bytes= CORRECT_STATE_1_COL_0_BYTES,\n",
    "            output_folder=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"S1_8_XOR_INCLUDED_{not INCLUDE_XOR}\".upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a2fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the specified word indices (0, 4, 8, 12)\n",
    "for word_index in [0, 4, 8, 12]:\n",
    "    # Iterate over the byte indices (0-3)\n",
    "    for byte_index in range(4):\n",
    "        # Call the function for each combination of word and byte index\n",
    "        process_and_plot_correlation_matrix_s1(\n",
    "            s1_word_index=word_index,\n",
    "            byte_index=byte_index,\n",
    "            show_plots=False,\n",
    "            save_plots=True,\n",
    "            zoom_plot=True,\n",
    "            list_correlations=True,\n",
    "            correct_s1_bytes= CORRECT_STATE_1_COL_0_BYTES,\n",
    "            output_folder=CPA_OUTPUT_FOLDER\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6397",
   "metadata": {},
   "source": [
    "## Success rate in bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89988c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges of S1 state bytes to analyze\n",
    "S1_BYTE_RANGES = [0, 4, 8, 12]\n",
    "\n",
    "# Initialize counters for correct bytes and correct bits\n",
    "correct_s1_bytes = 0\n",
    "correct_s1_bits = 0\n",
    "\n",
    "# Iterate over the specified S1 byte ranges\n",
    "for word_index in S1_BYTE_RANGES:\n",
    "    for byte_index in range(4):  # Each word has 4 bytes\n",
    "        # Get the obtained S1 byte and the correct S1 byte\n",
    "        obtained_byte = (FOUND_STATE_1_COL_0[word_index] >> (8 * byte_index)) & 0xFF\n",
    "        correct_byte = (CORRECT_STATE_1_COL_0[word_index] >> (8 * byte_index)) & 0xFF\n",
    "        \n",
    "        # Check if the entire byte matches\n",
    "        if obtained_byte == correct_byte:\n",
    "            correct_s1_bytes += 1\n",
    "        \n",
    "        # Count the number of matching bits in the byte\n",
    "        print(f\"S1 Byte (Word {word_index}, Byte {byte_index}): Obtained = {hex(obtained_byte)}, Correct = {hex(correct_byte)}\")\n",
    "        # Count the number of matching bits using XOR and bit counting\n",
    "        matching_bits = bin(~(obtained_byte ^ correct_byte) & 0xFF).count('1')\n",
    "        correct_s1_bits += matching_bits\n",
    "\n",
    "# Print the results\n",
    "total_s1_bytes = len(S1_BYTE_RANGES) * 4  # 4 bytes per word\n",
    "print(f\"Correct S1 Bytes: {correct_s1_bytes} / {total_s1_bytes}\")\n",
    "print(f\"Correct S1 Bits: {correct_s1_bits} / {total_s1_bytes * 8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211e711",
   "metadata": {},
   "source": [
    "## Experiment with S1_8 attack\n",
    "Usage of XOR in the LF influences the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732aebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BYTE_RANGES_S1_XMEGA[(8,0)] = (16500, 18500)\n",
    "BYTE_RANGES_S1_XMEGA[(8,1)] = (16500, 18500)\n",
    "BYTE_RANGES_S1_XMEGA[(8,2)] = (16500, 18500)\n",
    "BYTE_RANGES_S1_XMEGA[(8,3)] = (16500, 18500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba848b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPA_OUTPUT_FOLDER_EXPERIMENT = os.path.join(CPA_OUTPUT_FOLDER, f\"S1_8_XOR_INCLUDED_{not INCLUDE_XOR}\".upper())\n",
    "os.makedirs(CPA_OUTPUT_FOLDER_EXPERIMENT, exist_ok=True)  # Ensure the folder exists\n",
    "# Print the folder path for the output\n",
    "print(f\"Output folder for S1_8_EXPERIMENT: {CPA_OUTPUT_FOLDER_EXPERIMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfcdb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xor_included = not INCLUDE_XOR\n",
    "print(f\"INCLUDE_XOR = {xor_included}\")\n",
    "\n",
    "calculate_correlation_matrix_for_s1_words(\n",
    "    s1_word_indices=[8],\n",
    "    byte_indices=[0, 1, 2, 3],\n",
    "    num_traces=TRACES_USED,\n",
    "    include_XOR=xor_included,\n",
    "    trace_shift=0, \n",
    "    obtained_s1_col_0=CORRECT_STATE_1_COL_0\n",
    ")\n",
    "\n",
    "for word_index in [8]:\n",
    "    # Iterate over the byte indices (0-3)\n",
    "    for byte_index in range(4):\n",
    "        # Call the function for each combination of word and byte index\n",
    "        process_and_plot_correlation_matrix_s1(\n",
    "            s1_word_index=word_index,\n",
    "            byte_index=byte_index,\n",
    "            show_plots=False,\n",
    "            save_plots=False,\n",
    "            zoom_plot=False,\n",
    "            list_correlations=False,\n",
    "            correct_s1_bytes= CORRECT_STATE_1_COL_0_BYTES,\n",
    "            output_folder=None\n",
    "        )\n",
    "\n",
    "for word_index in [8]:\n",
    "    # Iterate over the byte indices (0-3)\n",
    "    for byte_index in range(4):\n",
    "        # Call the function for each combination of word and byte index\n",
    "        process_and_plot_correlation_matrix_s1(\n",
    "            s1_word_index=word_index,\n",
    "            byte_index=byte_index,\n",
    "            show_plots=False,\n",
    "            save_plots=True,\n",
    "            zoom_plot=True,\n",
    "            list_correlations=True,\n",
    "            correct_s1_bytes= CORRECT_STATE_1_COL_0_BYTES,\n",
    "            output_folder=CPA_OUTPUT_FOLDER_EXPERIMENT\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe7a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
